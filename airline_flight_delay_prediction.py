# -*- coding: utf-8 -*-
"""Airline flight delay prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Do3wFxt_lOhcsltN7ZWgzVImmvJeDMnN
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# import dataset
data = pd.read_csv(r'/content/flights-larger.csv')
data = data.sample(frac = 0.1, random_state = 42)
print(f"shape: {data.shape}")
data.head()

df = pd.DataFrame(data)
df

df.info

# Check missing value
df.isnull().sum().to_frame('NaN value').T

# check for null values in each column
null_columns = df.columns[df.isnull().any()]
print(df[null_columns].isnull().sum())

# check for null values in each row
null_rows = df[df.isnull().any(axis=1)]
print(null_rows)

# filling  null values in 'delay' column with random integers between -30 and 170
#df['delay'].fillna(np.random.randint(-30, 171), inplace=True)

# Remove the 'flight' column
df =  df.drop('flight', axis = 1)

# Remove records with missing 'delay' values
#flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')

# Remove records with missing values 
df = df.dropna()
print(df.count())

# check for null values in each column
null_columns = df.columns[df.isnull().any()]
print(df[null_columns].isnull().sum())

# check count of unique values in each columns
for col in df:
    print(f"{col}: {df[col].nunique()}")

# more details
df.describe(include=[np.number]).T

df.describe(include=[object]).T

# Create a figure with subplots
fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(14,10))
plt.subplots_adjust(wspace=0.25, hspace=0.4) # Adjust spacing between subplots

# Subplot 1: Bar chart of average delay time by carrier
sns.barplot(x='carrier', y='delay', data=df, ax=axs[0,0], palette='Blues_d')
axs[0,0].set_xlabel('Carrier', fontsize=12)
axs[0,0].set_ylabel('Average Delay (minutes)', fontsize=12)
axs[0,0].set_title('Average Delay Time by Carrier', fontsize=14)
axs[0,0].tick_params(axis='both', labelsize=10)
axs[0,0].grid(True, alpha=0.5, linestyle='--')

# Subplot 2: Line chart of average delay time by month
sns.lineplot(x='mon', y='delay', data=df, ax=axs[0,1], color='purple')
axs[0,1].set_xlabel('Month', fontsize=12)
axs[0,1].set_ylabel('Average Delay (minutes)', fontsize=12)
axs[0,1].set_title('Average Delay Time by Month', fontsize=14)
axs[0,1].tick_params(axis='both', labelsize=10)
axs[0,1].set_xticks(range(1,13))
axs[0,1].grid(True, alpha=0.5, linestyle='--')

# Subplot 3: Scatterplot of delay time by departure time
sns.scatterplot(x='depart', y='delay', data=df, ax=axs[1,0], hue='dow', palette='viridis')
axs[1,0].set_xlabel('Departure Time (decimal hour)', fontsize=12)
axs[1,0].set_ylabel('Delay (minutes)', fontsize=12)
axs[1,0].set_title('Delay Time by Departure Time', fontsize=14)
axs[1,0].tick_params(axis='both', labelsize=10)
axs[1,0].grid(True, alpha=0.5, linestyle='--')

# Subplot 4: Heatmap of average delay time by day of week and carrier
carrier_delay = df.pivot_table(values='delay', index='dow', columns='carrier')
sns.heatmap(carrier_delay, cmap='coolwarm', ax=axs[1,1], annot=True, fmt=".0f")
axs[1,1].set_xlabel('Carrier', fontsize=12)
axs[1,1].set_ylabel('Day of Week', fontsize=12)
axs[1,1].set_title('Average Delay Time by Day of Week and Carrier', fontsize=14)
axs[1,1].tick_params(axis='both', labelsize=10)

# check for duplicates based on all columns
duplicate_rows = df[df.duplicated()]
# print the duplicate rows
print(duplicate_rows)

corr_matrix = df.corr()

sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()

df.head()

#import plotly.graph_objects as go

#fig = go.Figure(df=[go.Scatter3d(x=df['depart'],
                                   #y=df['mon'],
                                   #z=df['org'],
                                   #mode='markers',
                                   #marker=dict(color=df['delay'], size=5))])

#fig.show()

# Convert columns 'mile' to 'km' and then drop it
# Convert miles to kilometers
df['km'] = df['mile'] * 1.60934

# Drop the miles column
df.drop('mile', axis=1, inplace=True)


# Create 'label' column indicating whether a flight is delayed or not
df['label'] = (df['delay'] >= 15).astype(int)

# Check first five records
df.head(5)

from sklearn.preprocessing import LabelEncoder
# Convert categorical columns to numeric using LabelEncoder
le = LabelEncoder()
df['carrier_idx'] = le.fit_transform(df['carrier'])
df['org_idx'] = le.fit_transform(df['org'])

# Preview first five records
df.head()

import pandas as pd

# create a new dataframe with the predictor variables
X = pd.concat([df['mon'], df['dom'], df['dow'], df['carrier_idx'], df['org_idx'], df['km'], df['depart'], df['duration']], axis=1)

# create a new dataframe with the target variable
y = df['delay']

# combine the predictor and target dataframes
df_flights= pd.concat([X, y], axis=1)

# rename the target column to 'label'
df_flights = df_flights.rename(columns={'delay': 'label'})

# check the resulting dataframe
print(df_flights.head())

#sseperating the label variable from the feature column
#y = df_flights['label'] 
##X = df_flights.drop('label', axis = 1)
#from sklearn.decomposition import PCA
#from sklearn.preprocessing import StandardScaler

#scaler = StandardScaler()
#scaled_data = scaler.fit_transform(X)

#pca = PCA(n_components = 2)
#X_pca = pca.fit_transform(scaled_data)

"""# Machine Learning Models

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score



# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df_flights.drop('label',axis = 1), df['label'], test_size=0.3, random_state=42)

# Define the machine learning models to train
models = [
    ('Random Forest', RandomForestClassifier(random_state=42)),
    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),
    ('SVM', SVC(random_state=42)),
    ('KNN', KNeighborsClassifier()),
    ('Neural Network', MLPClassifier(random_state=42))
    
]
#model = ('Random Forest', RandomForestClassifier(random_state=42))
# Train and evaluate each model
results = []
for name, model in models:
    print('Training', name)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    (name, 'accuracy:', accuracy)
    (name, 'precision:', precision)
    (name, 'recall:', recall)
    (name, 'F1 score:', f1) 
    results.append((name, accuracy, precision, recall, f1))

    #Create a DataFrame from the results and print it out
    df_results = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])
    print(df_results)

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import GradientBoostingClassifier

scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1_score': make_scorer(f1_score)
}

param_grid = {
    'learning_rate': [0.01, 0.1],
    'n_estimators': [100, 500],
    'max_depth': [3, 5]
}

gb = GradientBoostingClassifier()

grid_search = GridSearchCV(estimator=gb, param_grid=param_grid, scoring=scoring, refit='accuracy', cv=5)

grid_search.fit(X_train, y_train)

print(grid_search.best_params_)
print(grid_search.best_score_)

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report

# Instantiate the Gradient Boosting Classifier with the best hyperparameters
gb_clf = GradientBoostingClassifier(learning_rate=0.01, max_depth=5, n_estimators=500)

# Fit the classifier on the entire training dataset
gb_clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = gb_clf.predict(X_test)

# Evaluate the model on the test set
print(classification_report(y_test, y_pred))

import pickle

# save the model to disk
AFDP = 'finalized_model.pkl'
pickle.dump(gb_clf, open(AFDP, 'wb'))

# load the model from disk
loaded_model = pickle.load(open(AFDP, 'rb'))

# make a prediction using the loaded model
result = loaded_model.predict(X_test)